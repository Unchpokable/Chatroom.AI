syntax = "proto3";

package llmkernel;

// Запрос от клиента к LLM
message LLMRequest {
  string request_id = 1;      // UUID запроса для отслеживания
  string model = 2;           // Имя модели (например, "anthropic/claude-3.5-sonnet")
  string system_prompt = 3;   // Системный промпт
  string user_prompt = 4;     // Пользовательский промпт
  bool stream = 5;            // true = streaming, false = ждать полный ответ
}

// Подтверждение приёма запроса
message Ack {
  string request_id = 1;
  bool accepted = 2;          // true = запрос принят, false = ошибка
  string error_code = 3;      // Код ошибки (пусто если accepted=true)
  string error_message = 4;   // Описание ошибки
}

// Chunk контента при streaming
message StreamChunk {
  string request_id = 1;
  string content = 2;         // Часть сгенерированного текста
}

// Финальный ответ (завершение streaming или полный ответ без streaming)
message LLMResponse {
  string request_id = 1;
  string content = 2;         // Полный текст (для non-stream) или пусто (для stream)
  string finish_reason = 3;   // "stop", "length", "content_filter", etc.
  uint32 prompt_tokens = 4;   // Количество токенов в промпте
  uint32 completion_tokens = 5; // Количество токенов в ответе
}

// Обёртка для всех WebSocket сообщений
message WebSocketMessage {
  oneof payload {
    LLMRequest request = 1;
    Ack ack = 2;
    StreamChunk chunk = 3;
    LLMResponse response = 4;
  }
}
